Emotions are essential to comprehending human interactions. Efforts are being made to discover techniques that can mimic the human capacity to recognize emotions conveyed through facial expressions, variations in tone while speaking, and images of faces. Emotion Classifier is among these disciplines. This project reviews machine learning classification and deep learning algorithms for human expression recognition systems using multimodal signals. This work would assist individuals in forming relationships and is applicable in various fields, including the HCI (Human-Computer Interaction) and pharmaceutical industries. The speech and video inputs are selected and intend to develop a model that collects data from each respective data set and predicts the emotion class. The primary purpose of this project is to enable researchers to assess the feasibility of human-computer interfaces that are sensitive to a person's emotions. The reuse of a previously learned model on a new problem is known as transfer learning, which is popular in deep learning now since it can train deep neural networks with a small amount of data. In this paper, we have applied a Deep learning model, i.e. CNN, and compared it with the existing models such as Multilayer perceptron and Decision tree classifier. The study aims to approach and improve continuous human expression recognition via video and audio and report the most recent developments in this field. To improve the accuracy of the existing model for speech, we have used two different combinations of datasets, i.e. RAVDESS and TESS and accomplished 87.08% accuracy using CNN. For the facial expression model, we have used the FER 2013 dataset using a transfer learning algorithm, and the model reached an accuracy approx. 99% over seven classes.
